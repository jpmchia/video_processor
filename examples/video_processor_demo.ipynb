{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Processor with YOLO Demo\n",
    "\n",
    "This notebook demonstrates how to use the Video Processor with YOLO models for object detection in videos. The processor can automatically detect objects in videos and extract segments containing those objects.\n",
    "\n",
    "## Features\n",
    "- Automatic model downloading\n",
    "- Object detection in videos\n",
    "- Parallel video processing\n",
    "- Progress tracking\n",
    "- Memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/jpmchia/src/Ultralytics\n",
      "Successfully imported model manager\n",
      "Successfully imported video_processor\n",
      "Running in Jupyter: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the project root directory\n",
    "project_root = Path().absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Keep going up until we find the video_processor directory or reach the filesystem root\n",
    "while project_root != project_root.parent and not (project_root / \"video_processor\").exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add the project root to the Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Import the model manager (this should work as we've already fixed it)\n",
    "try:\n",
    "    from models import get_model, model_manager\n",
    "    print(\"Successfully imported model manager\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing models: {e}\")\n",
    "\n",
    "# Try importing the video_processor package\n",
    "try:\n",
    "    # This is the import that was failing\n",
    "    import video_processor\n",
    "    from video_processor.ui.jupyter_interface import main_jupyter\n",
    "    print(\"Successfully imported video_processor\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing video_processor: {e}\")\n",
    "    print(\"You may need to install the package or fix the Python path\")\n",
    "\n",
    "# Check if we're running in a Jupyter notebook\n",
    "try:\n",
    "    import IPython\n",
    "    is_jupyter = IPython.get_ipython().__class__.__name__ == 'ZMQInteractiveShell'\n",
    "    print(f\"Running in Jupyter: {is_jupyter}\")\n",
    "except (ImportError, AttributeError):\n",
    "    is_jupyter = False\n",
    "    print(\"Not running in Jupyter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Management\n",
    "\n",
    "The Video Processor uses YOLO models for object detection. Let's explore the available models and download one for our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available pre-trained models:\n",
      "  - yolov8n.pt\n",
      "  - yolov8s.pt\n",
      "  - yolov8m.pt\n",
      "  - yolov8l.pt\n",
      "  - yolov8x.pt\n",
      "  - yolo11n.pt\n",
      "  - yolo11s.pt\n",
      "  - yolo11m.pt\n",
      "  - yolo11l.pt\n",
      "  - yolo11x.pt\n",
      "\n",
      "Models will be stored in: /home/jpmchia/src/Ultralytics/video_processor/models/weights\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "print(\"Available pre-trained models:\")\n",
    "for model_name in model_manager.list_available_models():\n",
    "    print(f\"  - {model_name}\")\n",
    "\n",
    "# Show where models will be stored\n",
    "print(f\"\\nModels will be stored in: {model_manager.models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and loading model: yolov8n.pt\n",
      "(This will only download if the model isn't already cached)\n",
      "Model loaded in 1.54 seconds\n",
      "Model type: YOLO\n",
      "Model task: detect\n"
     ]
    }
   ],
   "source": [
    "# Download and load a model\n",
    "# We'll use yolov8n.pt which is small and fast for demonstration purposes\n",
    "model_name = \"yolov8n.pt\"\n",
    "print(f\"Downloading and loading model: {model_name}\")\n",
    "print(\"(This will only download if the model isn't already cached)\")\n",
    "\n",
    "start_time = time.time()\n",
    "model = get_model(model_name)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Model loaded in {elapsed:.2f} seconds\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Model task: {model.task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Video Processing Configuration\n",
    "\n",
    "Before processing videos, we need to configure the processor. Let's set up the configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing configuration:\n",
      "  - confidence: 0.35\n",
      "  - buffer_seconds: 5\n",
      "  - min_object_area_ratio: 0.002\n",
      "  - target_classes: [0, 1, 2, 3, 5, 7]\n",
      "  - roi_coords: None\n",
      "  - motion_threshold: 0.015\n",
      "  - skip_frames: 15\n",
      "  - resize_factor: 0.5\n",
      "  - adaptive_skip: True\n",
      "  - debug: True\n"
     ]
    }
   ],
   "source": [
    "# Configuration for video processing\n",
    "config = {\n",
    "    \"confidence\": 0.35,        # Confidence threshold for detections\n",
    "    \"buffer_seconds\": 5,       # Buffer seconds before and after detection\n",
    "    \"min_object_area_ratio\": 0.002,  # Minimum object area relative to frame\n",
    "    \"target_classes\": [0, 1, 2, 3, 5, 7],  # Person, bicycle, car, motorcycle, bus, truck\n",
    "    \"roi_coords\": None,        # Region of interest (None for full frame)\n",
    "    \"motion_threshold\": 0.015, # Motion detection threshold\n",
    "    \"skip_frames\": 15,         # Number of frames to skip between detections\n",
    "    \"resize_factor\": 0.5,      # Resize factor for processing (smaller = faster)\n",
    "    \"adaptive_skip\": True,     # Adaptively adjust frame skipping\n",
    "    \"debug\": True              # Enable debug information\n",
    "}\n",
    "\n",
    "print(\"Video processing configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the Jupyter Interface\n",
    "\n",
    "The Video Processor includes a Jupyter interface for interactive use. Let's launch it to process videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported main_jupyter\n",
      "Running in Jupyter: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path().absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import our modules\n",
    "from models import get_model, model_manager\n",
    "\n",
    "# Import the jupyter_interface directly from the ui module\n",
    "try:\n",
    "    # This is the correct import path based on your project structure\n",
    "    from ui.jupyter_interface import main_jupyter\n",
    "    print(\"Successfully imported main_jupyter\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing main_jupyter: {e}\")\n",
    "    \n",
    "# Check if we're running in a Jupyter notebook\n",
    "try:\n",
    "    import IPython\n",
    "    is_jupyter = IPython.get_ipython().__class__.__name__ == 'ZMQInteractiveShell'\n",
    "    print(f\"Running in Jupyter: {is_jupyter}\")\n",
    "except (ImportError, AttributeError):\n",
    "    is_jupyter = False\n",
    "    print(\"Not running in Jupyter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .jp-OutputArea-output, .jp-Cell-outputWrapper {\n",
       "                    background-color: #1e1e1e !important;\n",
       "                    color: #e0e0e0 !important;\n",
       "                }\n",
       "                \n",
       "                /* Style for all jupyter widgets */\n",
       "                .jupyter-widgets {\n",
       "                    background-color: #1e1e1e !important;\n",
       "                    color: #e0e0e0 !important;\n",
       "                }\n",
       "                \n",
       "                /* Target HTML elements inside widgets but preserve their colors */\n",
       "                .jupyter-widgets body,\n",
       "                .jupyter-widgets div,\n",
       "                .jupyter-widgets span,\n",
       "                .jupyter-widgets p,\n",
       "                .jupyter-widgets h1,\n",
       "                .jupyter-widgets h2,\n",
       "                .jupyter-widgets h3,\n",
       "                .jupyter-widgets h4,\n",
       "                .jupyter-widgets h5,\n",
       "                .jupyter-widgets h6,\n",
       "                .jupyter-widgets ul,\n",
       "                .jupyter-widgets ol,\n",
       "                .jupyter-widgets dl,\n",
       "                .jupyter-widgets pre,\n",
       "                .jupyter-widgets form,\n",
       "                .jupyter-widgets table,\n",
       "                .jupyter-widgets th,\n",
       "                .jupyter-widgets td {\n",
       "                    color: inherit;\n",
       "                }\n",
       "                \n",
       "                /* Preserve colors for status indicators */\n",
       "                .jupyter-widgets .text-success {\n",
       "                    color: #81c784 !important;\n",
       "                }\n",
       "                \n",
       "                .jupyter-widgets .text-info {\n",
       "                    color: #4fc3f7 !important;\n",
       "                }\n",
       "                \n",
       "                .jupyter-widgets .text-warning {\n",
       "                    color: #ffb74d !important;\n",
       "                }\n",
       "                \n",
       "                .jupyter-widgets .text-danger {\n",
       "                    color: #e57373 !important;\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a6646be9b140dab88cada43a1443f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h2>Video Processing with YOLO</h2>', layout=Layout(margin='10px 0'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd0e39e19c541478a851cabdab8a5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='/mnt/j/Bugs/LAIDG0025410XEDT', description='Base Directory:', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8b33f9f1084c86825845ceec91861e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='/mnt/j/Bugs/Processed', description='Output Directory:', layout=Layout(width='80%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5132f0dc72e847b8808ec5437d7abfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='YOLO Model:', layout=Layout(width='50%'), options=('yolo11n.pt', 'yolo11s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd857203af794c03b927725819f2f881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Subfolder:', layout=Layout(width='80%'), options=('19700101', '20250327',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa55a4643e3043bd922097e0ae998042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Process Subfolder', icon='play', layout=Layout(widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e943732c55445218291e4c5ee7f5af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-07 19:03:13,131 - INFO - Processing video: 000006_100.mp4\n",
      "2025-05-07 19:03:13,131 - INFO - Processing video: 000030_100.mp4\n",
      "2025-05-07 19:03:13,132 - INFO - Processing video: 000007_100.mp4\n",
      "2025-05-07 19:03:13,620 - ERROR - Could not open video file: /mnt/j/Bugs/LAIDG0025410XEDT/19700101/000030_100.mp4\n",
      "2025-05-07 19:03:13,620 - ERROR - Could not open video file: /mnt/j/Bugs/LAIDG0025410XEDT/19700101/000006_100.mp4\n",
      "2025-05-07 19:03:13,687 - INFO - Updated processing log for 000030_100.mp4: processed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-07 19:03:13,704 - INFO - Updated processing log for 000006_100.mp4: marked for retry\n",
      "2025-05-07 19:03:13,705 - ERROR - Error processing 000006_100.mp4: [Errno 2] No such file or directory: '/mnt/j/Bugs/LAIDG0025410XEDT/19700101/processing_log_temp.csv' -> '/mnt/j/Bugs/LAIDG0025410XEDT/19700101/processing_log.csv'\n",
      "2025-05-07 19:03:13,974 - INFO - Video properties: 2304x1296, 15.00fps, 3.13s\n",
      "2025-05-07 19:03:13,975 - INFO - Adaptive frame skipping: 15 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpmchia/src/Ultralytics/video_processor/core/video.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 389.7ms\n",
      "Speed: 49.0ms preprocess, 389.7ms inference, 538.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 16.3ms\n",
      "Speed: 1.6ms preprocess, 16.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpmchia/src/Ultralytics/video_processor/core/video.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'mp42isom'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [2304, 1296], 'bitrate': 728, 'fps': 15.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 8000, 'bitrate': 15, 'metadata': {'Metadata': '', 'handler_name': 'SoundHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 3.13, 'bitrate': 745, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [2304, 1296], 'video_bitrate': 728, 'video_fps': 15.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 8000, 'audio_bitrate': 15, 'video_duration': 3.13, 'video_n_frames': 46}\n",
      "/home/jpmchia/src/Ultralytics/video_processor/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i /mnt/j/Bugs/LAIDG0025410XEDT/19700101/000007_100.mp4 -loglevel error -f image2pipe -vf scale=2304:1296 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "2025-05-07 19:03:47,255 - ERROR - Error extracting segment 1: 'VideoFileClip' object has no attribute 'subclip'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-07 19:03:47,257 - INFO - Completed processing 000007_100.mp4: 0 segments extracted\n",
      "2025-05-07 19:03:47,264 - INFO - Updated processing log for 000007_100.mp4: processed successfully\n"
     ]
    }
   ],
   "source": [
    "# Launch the Jupyter interface\n",
    "# This will display widgets for selecting input/output directories, model, and other parameters\n",
    "main_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manual Video Processing Example\n",
    "\n",
    "If you prefer to process videos programmatically rather than using the interface, you can use the following code. This is useful for batch processing or integration into other workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory /path/to/your/videos does not exist. Please update the path.\n"
     ]
    }
   ],
   "source": [
    "# Import the processor function\n",
    "from video_processor.core.processor import process_subfolder\n",
    "\n",
    "# Define input and output directories\n",
    "# Replace these with your actual video directories\n",
    "input_dir = \"/path/to/your/videos\"\n",
    "output_dir = \"/path/to/output\"\n",
    "\n",
    "# Check if the input directory exists\n",
    "if not os.path.exists(input_dir):\n",
    "    print(f\"Input directory {input_dir} does not exist. Please update the path.\")\n",
    "else:\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process the videos\n",
    "    # Note: This code is commented out to prevent accidental execution\n",
    "    # Uncomment and update the paths to run it\n",
    "    \"\"\"\n",
    "    clips = process_subfolder(\n",
    "        input_dir,\n",
    "        output_dir,\n",
    "        model,\n",
    "        config,\n",
    "        max_workers=2,  # Adjust based on your system\n",
    "        memory_limit_percent=85\n",
    "    )\n",
    "    \n",
    "    print(f\"Processed videos and extracted {len(clips)} segments\")\n",
    "    for i, clip_path in enumerate(clips[:5]):  # Show first 5 clips\n",
    "        print(f\"  {i+1}. {os.path.basename(clip_path)}\")\n",
    "    if len(clips) > 5:\n",
    "        print(f\"  ... and {len(clips) - 5} more\")\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Manual processing code is ready but commented out.\")\n",
    "    print(\"Update the paths and uncomment the code to run it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Results\n",
    "\n",
    "After processing, you can visualize the results using the YOLO model's built-in visualization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of visualizing results on a single image or video frame\n",
    "# Replace with your actual image path\n",
    "image_path = \"/path/to/your/image.jpg\"\n",
    "\n",
    "# This code is commented out to prevent errors if the image doesn't exist\n",
    "# Uncomment and update the path to run it\n",
    "\"\"\"\n",
    "if os.path.exists(image_path):\n",
    "    # Run inference on the image\n",
    "    results = model(image_path)\n",
    "    \n",
    "    # Display the results\n",
    "    from IPython.display import display\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get the image with annotations\n",
    "    annotated_img = results[0].plot()\n",
    "    \n",
    "    # Convert to PIL Image and display\n",
    "    display(Image.fromarray(annotated_img))\n",
    "    \n",
    "    # Print detection information\n",
    "    print(f\"Detected {len(results[0].boxes)} objects\")\n",
    "    for i, box in enumerate(results[0].boxes):\n",
    "        cls = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        name = results[0].names[cls]\n",
    "        print(f\"  {i+1}. {name} (confidence: {conf:.2f})\")\n",
    "else:\n",
    "    print(f\"Image {image_path} does not exist. Please update the path.\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Visualization code is ready but commented out.\")\n",
    "print(\"Update the image path and uncomment the code to run it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "1. Set up the Video Processor environment\n",
    "2. Download and load YOLO models\n",
    "3. Configure video processing parameters\n",
    "4. Use the Jupyter interface for interactive processing\n",
    "5. Process videos programmatically\n",
    "6. Visualize detection results\n",
    "\n",
    "The Video Processor is a powerful tool for automatically detecting objects in videos and extracting relevant segments. It can be used for various applications such as surveillance, traffic monitoring, wildlife observation, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
